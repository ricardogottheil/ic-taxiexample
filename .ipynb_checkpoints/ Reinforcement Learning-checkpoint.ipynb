{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se acaba de hablar por teléfono con Uber, están teniendo algunos problemas con su nuevo auto sin conductor y quieren que lo arregles.\n",
    "\n",
    "# Hay 4 ubicaciones representadas por letras diferentes y es su trabajo recoger al pasajero en una ubicación y dejarlo en otra.\n",
    "\n",
    "# Carguemos nuestro primer entorno Taxi-v3.\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Podemos ver el estado de nuestro entorno utilizando render.\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ¡Ahora podemos ver nuestro entorno! \n",
    "# El cuadrado amarillo representa el taxi \n",
    "# \"|\" representa una pared \n",
    "# La letra de color azul representa el lugar de recogida del pasajero \n",
    "# La letra de color púrpura representa el destino del pasajero \n",
    "\n",
    "# Nota El taxi se volverá verde cuando un pasajero haya sido recogido. \n",
    "# Hay seis movimientos posibles que puede tomar un taxi: Arriba 0 Abajo 1 Derecha 2 Izquierda 3 Recogida 4 Devolución 5\n",
    "\n",
    "\n",
    "# Elige un estado aleatorio y renderízalo\n",
    "env.env.s = 240\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "# Nos movemos a la izquierda\n",
    "env.step(3)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "# Nos movemos a la izquierda\n",
    "env.step(3)\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1mR\u001b[0m\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "# Nos movemos hacia arriba\n",
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "# Nos movemos hacia arriba\n",
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n"
     ]
    }
   ],
   "source": [
    "# Recogemos al pasagero\n",
    "env.step(4)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[42m_\u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "# Nos movemos hacia abajo\n",
    "env.step(0)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "# Nos movemos hacia arriba\n",
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 0\n",
      "reward: 20\n",
      "done: True\n",
      "info: {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Estos resultados son el 'pan de cada día' del estado de aprendizaje de refuerzo. \n",
    "# Situación actual del agente en el entorno. \n",
    "# Recompensa. Comentarios de una acción realizada por el agente en el entorno. \n",
    "# Listo - Booleano que indica si el agente ha finalizado o completado su información del entorno. \n",
    "# Diagnóstico. información sobre la última acción de los agentes. \n",
    "\n",
    "# Recibe +20 puntos por una entrega exitosa y pierde 1 punto por cada paso de tiempo que tome. \n",
    "# También hay una penalización de 10 puntos por acciones ilegales de recogida y devolución.\n",
    "\n",
    "#Dejamos el pasajero \n",
    "\n",
    "state, reward, done, info = env.step(5)\n",
    "print('state: {}'.format(state))\n",
    "print('reward: {}'.format(reward))\n",
    "print('done: {}'.format(done))\n",
    "print('info: {}'.format(info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La conducción aleatoria tomó 2117 pasos para completar un viaje\n"
     ]
    }
   ],
   "source": [
    "#La recompensa es fácil de malinterpretar, normalmente es positiva, sin embargo, en el aprendizaje por refuerzo puede ser positiva o negativa.\n",
    "\n",
    "#Positivo\n",
    "\n",
    "# - El agente recibe +20 puntos por una entrega exitosa\n",
    "\n",
    "\n",
    "#Negativo\n",
    "\n",
    "# - El agente recibe -10 puntos por cada error que comete al recoger o dejar a un pasajero\n",
    "# - El agente recibe -1 punto por cada paso que realiza.\n",
    "\n",
    "\n",
    "# Cada respiro que tomas\n",
    "\n",
    "# Cada movimiento que haces\n",
    "\n",
    "# Cada vínculo que rompas\n",
    "\n",
    "# Cada paso que das\n",
    "\n",
    "# Te miraré recompensándote (con -1)\n",
    "\n",
    "# La razón por la que damos una recompensa negativa en cada turno, básicamente obliga al agente a encontrar la solución más rápida posible.\n",
    "\n",
    "# ¿Ahora que?\n",
    "\n",
    "# Bueno, tenemos el agente, el entorno y las recompensas. Vamos a explorar.\n",
    "\n",
    "# Cuando construimos cualquier modelo de aprendizaje automático, queremos comenzar de manera simple.\n",
    "\n",
    "# Sabemos que nuestro objetivo es recoger y llevar a nuestro pasajero a su destino, seleccionemos acciones aleatorias hasta que lo hagamos bien.\n",
    "\n",
    "# Use env.action_space.sample () para realizar acciones aleatorias hasta que tenga un viaje exitoso.\n",
    "\n",
    "# Piense cómo se recompensa un viaje exitoso.\n",
    "\n",
    "\n",
    "state = env.reset()\n",
    "reward = None\n",
    "steps = 0\n",
    "\n",
    "while reward != 20:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    steps += 1\n",
    "\n",
    "print('La conducción aleatoria tomó {} pasos para completar un viaje'.format(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAUGUlEQVR4nO3df5BdZ33f8fen8q8Gu5GMNtRjW8hOPRncFmRnx4axB0QThOxJrXSGmUqlICgezVC7zY+mHbtMcWs60/zoJAyNg60mqkkm2CSAE5UxMSrgupTa0QqEf8ZYCLvWyK02CJsQGFyZb/+4R+316u7ukfZKu/v4/Zo5c895nufc8zyro88enXuunlQVkqR2/ZXF7oAk6eQy6CWpcQa9JDXOoJekxhn0ktS40xa7A6OsXr261q5du9jdkKRlY8+ePX9eVROj6pZk0K9du5apqanF7oYkLRtJnpmtzls3ktQ4g16SGmfQS1LjDHpJapxBL0mNmzfok1yY5ItJnkjyWJKfG9EmST6SZF+Sh5NcPlS3NclT3bJ13AOQJM2tz+OVR4B/VlVfSXIOsCfJrqp6fKjNNcAl3XIl8FHgyiTnArcAk0B1++6sqm+PdRSSpFnNe0VfVc9V1Ve69b8AngDOn9FsE/C7NfAgsDLJecDbgV1VdbgL913AxrGOQJI0p+O6R59kLXAZ8NCMqvOBZ4e2D3Rls5WPeu9tSaaSTE1PTx9PtyRJc+gd9EnOBj4F/HxVfWdm9Yhdao7yYwurtlfVZFVNTkyM/BavJOkE9Ar6JKczCPnfr6pPj2hyALhwaPsC4OAc5ZKkU6TPUzcBfgd4oqp+fZZmO4F3d0/fvBF4oaqeA+4DNiRZlWQVsKErkySdIn2eurkKeBfwSJK9Xdm/BNYAVNXtwL3AtcA+4HvAe7u6w0k+BOzu9ru1qg6Pr/uSpPnMG/RV9SVG32sfblPADbPU7QB2nFDvJEkL5jdjJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNm3fikSQ7gJ8BDlXV3xpR/8+Bdw693+uAiW52qaeBvwBeAo5U1eS4Oi5J6qfPFf2dwMbZKqvq16pqXVWtA24G/uuM6QLf2tUb8pK0COYN+qp6AOg7z+sW4K4F9UiSNFZju0ef5EcYXPl/aqi4gM8l2ZNk2zz7b0sylWRqenp6XN2SpFe8cX4Y+3eB/z7jts1VVXU5cA1wQ5I3z7ZzVW2vqsmqmpyYmBhjtyTplW2cQb+ZGbdtqupg93oIuAe4YozHkyT1MJagT/KjwFuAPx4qe1WSc46uAxuAR8dxPElSf30er7wLWA+sTnIAuAU4HaCqbu+a/T3gc1X1l0O7vga4J8nR43y8qv5kfF2XJPUxb9BX1ZYebe5k8BjmcNl+4A0n2jFJ0nj4zVhJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuPmDfokO5IcSjJyGsAk65O8kGRvt3xwqG5jkieT7Ety0zg7Lknqp88V/Z3Axnna/LeqWtcttwIkWQHcBlwDXApsSXLpQjorSTp+8wZ9VT0AHD6B974C2FdV+6vqReBuYNMJvI8kaQHmnTO2pzcl+RpwEPilqnoMOB94dqjNAeDK2d4gyTZgG8CaNWsW3KGVK1fy3e9+l6uvvpr777+/937r168HOGaf9evXs3fvXgDWrVsHwN69e49Zn+tYK1euBOD555+f81jz1R2vo8c92tdRY5tZPnO8999//8vajVqHY38OxzOO4/n5jNK3/Xw/9y996UucffbZ/68fCznW8bZdqLnO3z7lw3+Wo9rP3G++dgt1Kn924z7uyTp/T8Q4Poz9CvDaqnoD8B+AP+rKM6JtzfYmVbW9qiaranJiYmIM3ZIkwRiCvqq+U1Xf7dbvBU5PsprBFfyFQ00vYHDFL0k6hRYc9En+epJ061d07/ktYDdwSZKLkpwBbAZ2LvR4kqTjM+89+iR3AeuB1UkOALcApwNU1e3AO4D3JzkCfB/YXFUFHElyI3AfsALY0d27lySdQvMGfVVtmaf+N4HfnKXuXuDeE+uaJGkc/GasJDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjZs36JPsSHIoyaOz1L8zycPd8uUkbxiqezrJI0n2JpkaZ8clSf30uaK/E9g4R/03gbdU1euBDwHbZ9S/tarWVdXkiXVRkrQQfWaYeiDJ2jnqvzy0+SCDScAlSUvEuO/Rvw/47NB2AZ9LsifJtrl2TLItyVSSqenp6TF3S5Jeuea9ou8ryVsZBP3VQ8VXVdXBJD8G7EryZ1X1wKj9q2o73W2fycnJGle/JOmVbixX9EleD/w2sKmqvnW0vKoOdq+HgHuAK8ZxPElSfwsO+iRrgE8D76qqrw+VvyrJOUfXgQ3AyCd3JEknz7y3bpLcBawHVic5ANwCnA5QVbcDHwReDfxWEoAj3RM2rwHu6cpOAz5eVX9yEsYgSZpDn6dutsxTfz1w/Yjy/cAbjt1DknQq+c1YSWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjegV9kh1JDiUZORVgBj6SZF+Sh5NcPlS3NclT3bJ1XB2XJPXT94r+TmDjHPXXAJd0yzbgowBJzmUw9eCVDCYGvyXJqhPtrCTp+PUK+qp6ADg8R5NNwO/WwIPAyiTnAW8HdlXV4ar6NrCLuX9hSJLGbFz36M8Hnh3aPtCVzVZ+jCTbkkwlmZqenh5TtyRJ4wr6jCirOcqPLazaXlWTVTU5MTExpm5JksYV9AeAC4e2LwAOzlEuSTpFxhX0O4F3d0/fvBF4oaqeA+4DNiRZ1X0Iu6ErkySdIqf1aZTkLmA9sDrJAQZP0pwOUFW3A/cC1wL7gO8B7+3qDif5ELC7e6tbq2quD3UlSWPWK+irass89QXcMEvdDmDH8XdNkjQOfjNWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktS4XkGfZGOSJ5PsS3LTiPrfSLK3W76e5PmhupeG6naOs/OSpPnNO8NUkhXAbcDbGEz2vTvJzqp6/GibqvqFofb/BLhs6C2+X1XrxtdlSdLx6HNFfwWwr6r2V9WLwN3ApjnabwHuGkfnJEkL1yfozweeHdo+0JUdI8lrgYuALwwVn5VkKsmDSX52toMk2da1m5qenu7RLUlSH32CPiPKapa2m4FPVtVLQ2VrqmoS+AfAh5P8+Kgdq2p7VU1W1eTExESPbkmS+ugT9AeAC4e2LwAOztJ2MzNu21TVwe51P3A/L79/L0k6yfoE/W7gkiQXJTmDQZgf8/RMkp8AVgH/Y6hsVZIzu/XVwFXA4zP3lSSdPPM+dVNVR5LcCNwHrAB2VNVjSW4FpqrqaOhvAe6uquHbOq8D7kjyQwa/VH55+GkdSdLJN2/QA1TVvcC9M8o+OGP7X4/Y78vA315A/yRJC+Q3YyWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWpcr6BPsjHJk0n2JblpRP17kkwn2dst1w/VbU3yVLdsHWfnJUnzm3fikSQrgNuAtzGYP3Z3kp0jZor6RFXdOGPfc4FbgEkGE4rv6fb99lh6L0maV58r+iuAfVW1v6peBO4GNvV8/7cDu6rqcBfuu4CNJ9ZVSdKJyMuneB3RIHkHsLGqru+23wVcOXz1nuQ9wL8DpoGvA79QVc8m+SXgrKr6t127fwV8v6r+/YjjbAO2AaxZs+Ynn3nmmTEMT5JeGZLsqarJUXV9rugzomzmb4f/DKytqtcD/wX42HHsOyis2l5Vk1U1OTEx0aNbkqQ++gT9AeDCoe0LgIPDDarqW1X1g27zPwI/2XdfSdLJ1SfodwOXJLkoyRnAZmDncIMk5w1tXgc80a3fB2xIsirJKmBDVyZJOkXmfeqmqo4kuZFBQK8AdlTVY0luBaaqaifwT5NcBxwBDgPv6fY9nORDDH5ZANxaVYdPwjgkSbOY98PYxTA5OVlTU1OL3Q1JWjYW+mGsJGkZM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqXK+gT7IxyZNJ9iW5aUT9LyZ5PMnDST6f5LVDdS8l2dstO2fuK0k6ueadSjDJCuA24G0MJvvenWRnVT0+1OyrwGRVfS/J+4FfBf5+V/f9qlo35n5Lknrqc0V/BbCvqvZX1YvA3cCm4QZV9cWq+l63+SBwwXi7KUk6UX2C/nzg2aHtA13ZbN4HfHZo+6wkU0keTPKzs+2UZFvXbmp6erpHtyRJfcx76wbIiLKRM4on+YfAJPCWoeI1VXUwycXAF5I8UlXfOOYNq7YD22EwOXiPfkmSeuhzRX8AuHBo+wLg4MxGSX4a+ABwXVX94Gh5VR3sXvcD9wOXLaC/kqTj1CfodwOXJLkoyRnAZuBlT88kuQy4g0HIHxoqX5XkzG59NXAVMPwhriTpJJv31k1VHUlyI3AfsALYUVWPJbkVmKqqncCvAWcDf5gE4H9W1XXA64A7kvyQwS+VX57xtI4k6SRL1dK7HT45OVlTU1OL3Q1JWjaS7KmqyVF1fjNWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktS4XkGfZGOSJ5PsS3LTiPozk3yiq38oydqhupu78ieTvH18XZck9TFv0CdZAdwGXANcCmxJcumMZu8Dvl1VfwP4DeBXun0vZTDH7N8ENgK/1b2fJOkU6XNFfwWwr6r2V9WLwN3AphltNgEf69Y/CfxUBpPHbgLurqofVNU3gX3d+0mSTpE+QX8+8OzQ9oGubGSbqjoCvAC8uue+ACTZlmQqydT09HS/3kuS5tUn6DOibOaM4rO16bPvoLBqe1VNVtXkxMREj25JkvroE/QHgAuHti8ADs7WJslpwI8Ch3vuK0k6ifoE/W7gkiQXJTmDwYerO2e02Qls7dbfAXyhqqor39w9lXMRcAnwp+PpuiSpj9Pma1BVR5LcCNwHrAB2VNVjSW4FpqpqJ/A7wO8l2cfgSn5zt+9jSf4AeBw4AtxQVS+dpLFIkkbI4MJ7aZmcnKypqanF7oYkLRtJ9lTV5Kg6vxkrSY0z6CWpcQa9JDXOoJekxi3JD2OTTAPPnODuq4E/H2N3FoNjWBocw9LgGPp5bVWN/Lbpkgz6hUgyNdsnz8uFY1gaHMPS4BgWzls3ktQ4g16SGtdi0G9f7A6MgWNYGhzD0uAYFqi5e/SSpJdr8YpekjTEoJekxjUT9PNNYL6YkuxIcijJo0Nl5ybZleSp7nVVV54kH+nG8XCSy4f22dq1fyrJ1lHHOoljuDDJF5M8keSxJD+33MaR5Kwkf5rka90Y/k1XflE3qf1T3ST3Z3TlS3bS+yQrknw1yWeW4xiSPJ3kkSR7k0x1ZcvmXOqOvTLJJ5P8Wff34k1LdgxVtewXBv998jeAi4EzgK8Bly52v4b692bgcuDRobJfBW7q1m8CfqVbvxb4LIPZud4IPNSVnwvs715XdeurTuEYzgMu79bPAb7OYLL4ZTOOri9nd+unAw91ffsDYHNXfjvw/m79HwO3d+ubgU9065d259iZwEXdubfiFJ9Tvwh8HPhMt72sxgA8DayeUbZszqXu+B8Dru/WzwBWLtUxnLIT8yT/wN8E3De0fTNw82L3a0Yf1/LyoH8SOK9bPw94slu/A9gysx2wBbhjqPxl7RZhPH8MvG25jgP4EeArwJUMvrF42sxzicEcDG/q1k/r2mXm+TXc7hT1/QLg88DfAT7T9Wm5jeFpjg36ZXMuAX8N+CbdAy1LfQyt3LrpPQn5EvKaqnoOoHv9sa58trEsmTF2//y/jMEV8bIaR3fLYy9wCNjF4Er2+RpMaj+zPwue9P4k+TDwL4AfdtuvZvmNoYDPJdmTZFtXtpzOpYuBaeA/dbfQfjvJq1iiY2gl6HtPQr4MLHii9ZMpydnAp4Cfr6rvzNV0RNmij6OqXqqqdQyuiq8AXjdHf5bcGJL8DHCoqvYMF8/RnyU3hs5VVXU5cA1wQ5I3z9F2KY7hNAa3Yz9aVZcBf8ngVs1sFnUMrQT9cpyE/H8nOQ+gez3Ulc82lkUfY5LTGYT871fVp7viZTcOgKp6Hrifwf3SlRlMaj+zP0tx0vurgOuSPA3czeD2zYdZXmOgqg52r4eAexj80l1O59IB4EBVPdRtf5JB8C/JMbQS9H0mMF9qhidU38rgnvfR8nd3n9K/EXih+yfgfcCGJKu6T/I3dGWnRJIwmBv4iar69aGqZTOOJBNJVnbrfxX4aeAJ4IsMJrUfNYYlNel9Vd1cVRdU1VoG5/kXquqdy2kMSV6V5Jyj6wzOgUdZRudSVf0v4NkkP9EV/RSDubGX5hhO1Ycvp+DDkWsZPAnyDeADi92fGX27C3gO+D8MfoO/j8F90s8DT3Wv53ZtA9zWjeMRYHLoff4RsK9b3nuKx3A1g39SPgzs7ZZrl9M4gNcDX+3G8Cjwwa78YgYhtw/4Q+DMrvysbntfV3/x0Ht9oBvbk8A1i3Reref/P3WzbMbQ9fVr3fLY0b+vy+lc6o69Dpjqzqc/YvDUzJIcg/8FgiQ1rpVbN5KkWRj0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXH/F8QXZzlRD0MGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número promedio de pasos para una unidad aleatoria 2270.5510204081634\n"
     ]
    }
   ],
   "source": [
    "# Ejecútelo unas cuantas veces más y vea cuánto tarda.\n",
    "\n",
    "random_driving_store = []\n",
    "for i in range(1,50):\n",
    "    state = env.reset()\n",
    "    reward = None\n",
    "    steps = 0\n",
    "\n",
    "    while reward != 20:\n",
    "        state, reward, done, info = env.step(env.action_space.sample())\n",
    "        steps += 1\n",
    "\n",
    "    random_driving_store.append(steps)\n",
    "    \n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure()\n",
    "plt.hlines(0.5,0.5,2)  # Draw a horizontal line\n",
    "plt.eventplot(random_driving_store, orientation='horizontal', colors='k')\n",
    "\n",
    "plt.show()\n",
    "print('Número promedio de pasos para una unidad aleatoria {}'.format(np.mean(random_driving_store)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible states: 500\n",
      "number of possible actions: 6\n"
     ]
    }
   ],
   "source": [
    "# No hay sorpresas aquí, asignar una acción aleatoria no llevará a nuestro taxi a ninguna parte.\n",
    "\n",
    "# Necesitamos una forma para que nuestro agente aprenda a maniobrar en las calles del gimnasio. Intentemos construir una Q-table.\n",
    "\n",
    "# Piense en una tabla Q inicialmente como un mapa en blanco del entorno. Su agente navegará por el espacio y actualizará la tabla con todo lo que encuentre interesante.\n",
    "\n",
    "# Con cada iteración, el agente obtiene cada vez más información sobre el entorno. En poco tiempo, su agente está zumbando por el medio ambiente.\n",
    "\n",
    "\n",
    "# La tabla Q se compone de filas iguales al número de estados y columnas iguales al número de acciones.\n",
    "\n",
    "# El valor en la celda indica la recompensa esperada para una acción dado ese estado.\n",
    "\n",
    "# Construyamos nuestra Q-table \n",
    "\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print('number of possible states: {}'.format(state_size))\n",
    "print('number of possible actions: {}'.format(action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora tenemos las dimensiones de nuestra tabla Q, inicialicemos nuestra tabla Q como un lienzo en blanco (todos ceros)\n",
    "\n",
    "Q = np.zeros([state_size, action_size])\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "learning_rate = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward for this episode: -496\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "total_reward, reward = 1,0\n",
    "state = env.reset()\n",
    "while done != True: # Keeps making actions until episode completes\n",
    "        action = np.argmax(Q[state]) # Finds the action with the greatest reward. TIP each state is a row in the Q-table, find the best action at this state by finding the max value\n",
    "        new_state, reward, done, info = env.step(action) #Takes the action with the greatest reward\n",
    "        Q[state, action] += learning_rate * (reward + np.max(Q[new_state]) - Q[state, action]) # Updates our Q-table based on the state and actions. TIP If your stuck have a look at this pseudo code below\n",
    "        total_reward += reward # Update our total reward\n",
    "        state = new_state # Update our current state\n",
    "#         env.render() # Print the current agent-environment interaction\n",
    "print('Total reward for this episode: {}'.format(total_reward))\n",
    "\n",
    "\n",
    "# New Q value = Current Q value + learning rate * (Reward + (maximum value of new state) — Current Q value )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 Total Reward: -22\n",
      "Episode 100 Total Reward: -323\n",
      "Episode 150 Total Reward: -54\n",
      "Episode 200 Total Reward: 13\n",
      "Episode 250 Total Reward: 12\n",
      "Episode 300 Total Reward: 11\n",
      "Episode 350 Total Reward: 7\n",
      "Episode 400 Total Reward: 8\n",
      "Episode 450 Total Reward: 8\n",
      "Episode 500 Total Reward: 4\n",
      "Episode 550 Total Reward: 8\n",
      "Episode 600 Total Reward: 14\n",
      "Episode 650 Total Reward: 5\n",
      "Episode 700 Total Reward: 10\n",
      "Episode 750 Total Reward: 9\n",
      "Episode 800 Total Reward: 0\n",
      "Episode 850 Total Reward: 7\n",
      "Episode 900 Total Reward: 7\n",
      "Episode 950 Total Reward: 6\n",
      "Episode 1000 Total Reward: 3\n",
      "Episode 1050 Total Reward: 7\n",
      "Episode 1100 Total Reward: 5\n",
      "Episode 1150 Total Reward: 5\n",
      "Episode 1200 Total Reward: 12\n",
      "Episode 1250 Total Reward: 6\n",
      "Episode 1300 Total Reward: 7\n",
      "Episode 1350 Total Reward: 10\n",
      "Episode 1400 Total Reward: 4\n",
      "Episode 1450 Total Reward: 10\n",
      "Episode 1500 Total Reward: 8\n",
      "Episode 1550 Total Reward: 7\n",
      "Episode 1600 Total Reward: 11\n",
      "Episode 1650 Total Reward: 5\n",
      "Episode 1700 Total Reward: 5\n",
      "Episode 1750 Total Reward: 9\n",
      "Episode 1800 Total Reward: 7\n",
      "Episode 1850 Total Reward: 7\n",
      "Episode 1900 Total Reward: 10\n",
      "Episode 1950 Total Reward: 10\n",
      "Episode 2000 Total Reward: 8\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros([state_size, action_size])\n",
    "\n",
    "total_reward = 0\n",
    "learning_rate = 0.7\n",
    "\n",
    "for episode in range(1,2001):\n",
    "    done = False\n",
    "    total_reward, reward = 0,0\n",
    "    state = env.reset()\n",
    "    while done != True:\n",
    "        action = np.argmax(Q[state])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        Q[state, action] += learning_rate * (reward + np.max(Q[new_state]) - Q[state, action])\n",
    "        total_reward += reward\n",
    "        state = new_state   \n",
    "    if episode % 50 == 0:\n",
    "        print('Episode {} Total Reward: {}'.format(episode, total_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
